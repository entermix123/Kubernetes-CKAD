CONTENT

Section 6 106. Practice Test - CornJobs
Section 6 93. Practice Test - Labels, Selectors
Section 6 94. Rolling Updates & Rollback in Deployments
Section 6 95. Updating a Deployment
Section 6 96. Demo Deployment
Section 6 98. Practice Test - Rolling Updates & Rollback
Section 6 99. Deployment Strategy - Blue Green
Section 6 100. Deployment Strategy - Canary
Section 6 102. Practice Test - Deployment Strategies
Section 6 103. Jobs
Section 6 104. CornJobs
Section 6 106. Practice Test - Jobs and CornJobs


===============================================
Section 6 91. Labels, Selectors and Annotations
===============================================

Labels and Selectors give us the possibility to group and filter object by different criteria.

Labels are properties of every object.
Selectors are the criteria we are filtering on.

example for pod-definition.yaml file
------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    app: App1
    funtion: Front-End

spec:
  containers:
  -  name: simple-webapp
     image: simple-webapp
     ports:
     - containerPort: 8080
------------------------------------------

Show pod filtered with selector
	terminal --> kubectl get pods --selector app=App1




Example for replicaset
----------------------

replicaset-definition.yaml
------------------------------------------
apiVersion: v1
kind: ReplicaSet
metadata:
  name: simple-webapp
  labels:
    app: App1
    funtion: Front-End

spec:
  replicas: 3
  selector:
    matchLabels:
      app: App1			# this label have to match with template laabel
  template:
    metadata:
      labels:
        app: App1		# this labels have to cmatch with matchLabel
	function: Front-end
    spec:
      containers:
      - name: simple-webapp
        image: simple-webapp
------------------------------------------

Only if 2 labels in soec match, replicaset is created succeffully. 




example for services
--------------------

service-definition.yaml file
------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: App1		# this label must match replicaset template label
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
------------------------------------------





Annotations - are used for additionla information and interpretation purposes
-----------


replicaset-definition.yaml
------------------------------------------
apiVersion: v1
kind: ReplicaSet
metadata:
  name: simple-webapp
  labels:
    app: App1
    funtion: Front-End
  annotations: 
      buildversion: 1.34	# annotation information

spec:
  replicas: 3
  selector:
    matchLabels:
      app: App1			
  template:
    metadata:
      labels:
        app: App1		
	function: Front-end
    spec:
      containers:
      - name: simple-webapp
        image: simple-webapp
------------------------------------------





===============================================
Section 6 93. Practice Test - Labels, Selectors
===============================================

1. We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the dev environment (env)?
--------------------------------------------------------------------------------------------------------------------------------
Use selectors to filter the output

Show all object (pods) in dev environment
	terminal --> kubectl get all --selector env=prod --no-headers

	# kubectl				- common kubernetes command
	# get					- used action
	# all					- object kind
	# --selector env=prod			- set selector for environment
	# --no-headers				- exclude headers of the result table

Show count of all filtered objects
	terminal --> kubectl get all --selector env=dev --no-headers | wc -l

	# kubectl				- common kubernetes command
	# get					- used action
	# all					- object kind
	# --selector env=dev			- set selector for environment
	# --no-headers				- exclude headers of the result table
	# | wc -l				- show count of lines in the result

Show all pods in 'dev' environment
	terminal --> k get pods --selector env=dev

	# result:
	NAME          READY   STATUS    RESTARTS   AGE
	app-1-525l2   1/1     Running   0          7m5s
	app-1-9nt54   1/1     Running   0          7m5s
	app-1-mf5wv   1/1     Running   0          7m5s
	db-1-72vr5    1/1     Running   0          7m5s
	db-1-hxjjj    1/1     Running   0          7m5s
	db-1-njc7d    1/1     Running   0          7m5s
	db-1-wjfzv    1/1     Running   0          7m5s

Show count of the pods in 'dev' environment
	terminal --> k get pods --selector env=dev --no-headers | wc -l

	# result: 7

- choose '7' as answer



2. How many PODs are in the finance business unit (bu)?
-------------------------------------------------------

Show pods in bu finance
	terminal --> kubectl get pods --selector bu=finance

	# kubectl				- common kubernetes command
	# get					- used action
	# pods					- object kind
	# --selector bu=finance			- set selector for business unit


Show count
	terminal --> kubectl get pods --selector bu=finance --no-headers | wc -l

	# kubectl				- common kubernetes command
	# get					- used action
	# pods					- object kind
	# --selector bu=finance			- set selector for environment
	# --no-headers				- exclude headers of the result table
	# | wc -l				- show count of lines in the result

- choose '6' as answer



3. How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
--------------------------------------------------------------------------------------------------

Show all object in prod environment
	terminal --> kubectl get all --selector env=prod --no-headers | wc -l

	# kubectl				- common kubernetes command
	# get					- used action
	# all					- object kind
	# --selector env=prod			- set selector for environment
	# --no-headers				- exclude headers of the result table
	# | wc -l				- show count of lines in the result

- choose '7' as answer



4. Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
-----------------------------------------------------------------------------------------------

Show the pod in prod environment, finance bu and frontend tier
	terminal --> kubectl get pods --selector env=prod,bu=finance,tier=frontend

	# kubectl				- common kubernetes command
	# get					- used action
	# all					- object kind
	# --selector 				- selector
	# env=prod				- env=rpod selector
	# bu=finance				- bu=finance selector
	# tier=frontend				- tier-frontend selector

	# result:
	NAME          READY   STATUS    RESTARTS   AGE
	app-1-zzxdf   1/1     Running   0          11m

- choose 'app-1-zzxdf' as answer



5. A ReplicaSet definition file is given replicaset-definition-1.yaml. Attempt to create the replicaset; you will encounter an issue with the file. Try to fix it.
------------------------------------------------------------------------------------------------------------------------------
Once you fix the issue, create the replicaset from the definition file.

Trye to create the replicaset
	ternubal --> k create -f replicaset-definition-1.yaml

# result:
The ReplicaSet "replicaset-1" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"tier":"nginx"}: `selector` does not match template `labels`


Print the replicaset definition file
	terminal --> cat replicaset-definition-1.yaml

replicaset-definition-1.yaml
-----------------------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
   name: replicaset-1
spec:
   replicas: 2
   selector:
      matchLabels:
        tier: front-end			# this is different from template labels
   template:
     metadata:
       labels:
        tier: nginx			# this is different from replicaset selector
     spec:
       containers:
       - name: nginx
         image: nginx
-----------------------------



Edit the replicaset definition file
	terminal --> vi replicaset-definition-1.yaml

replicaset-definition-1.yaml
-----------------------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
   name: replicaset-1
spec:
   replicas: 2
   selector:
      matchLabels:
        tier: nginx
   template:
     metadata:
       labels:
        tier: nginx
     spec:
       containers:
       - name: nginx
         image: nginx
-----------------------------
save changes - escape, :wq!, enter


Try to create the replicaset again
	ternubal --> k create -f replicaset-definition-1.yaml

	# result: replicaset.apps/replicaset-1 created


- cklick 'Check' button




=======================================================
Section 6 94. Rolling Updates & Rollback in Deployments
=======================================================


Explain Rollout and Versioning
==============================

When we first create a deployment (example: nginx:1.7.0) it creates a Rollout. A new rollout create a new deployment revision - Revision 1.

When the application container is updated (nginx:1.7.1), a new Rollout is triggered and a new deployment revision is created - Revision 2.


Rollout Commands
----------------

Show rollouts status
	terminal --> kubectl rollout status deployment/myapp-deployment

	# kubectl 				- common kubernetes command
	# rollout status			- information about obejct required
	# deployment/myapp-deployment		- taget object


Show revisions and history of rollout
	terminal --> kubectl rollout history deployment/myapp-deployment

	# kubectl 				- common kubernetes command
	# rollout history			- information about obejct required
	# deployment/myapp-deployment		- taget object



Deployment Strategies
=====================

1. Reacreate Strategy - Destroy all application pods at ones and create all pods again with the new version
	- this creates a downtime of the app, because there is period of no working application 
	- not the default deployment strategy
	- not recommended


2. Rolling Update Strategy - take down and update each pod one by one
	- this strategy do not have downtime period
	- default kubernetes strategy
	- recommended


How we update deployment
========================

Make changes to the deployemnt-definition file (declarative approach)

deployment-definition.yaml
------------------------------------------------
apiVersion: v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
	app: myapp
	type: front-end
    spec:
      containers:
      - name: nginx-container
	image: nginx:1.7.1		# add version
  replicas: 3
  selector:
    matchLabels:
       type: front-end
------------------------------------------------


Update command we use usually
	temrinal --> kubectl apply -f deployment-definition.yaml

	# result: deployment "myapp-deployment" configured

	# new rollout is triggered 
	# new revision of the deployment is created


Image update command (imperative approach)
	terminal --> kubectl set image deployemnt/myapp-deployment nginx-container=nginx:1.9.1

	# this command DO NOT update deployemnt-definition.yaml file !!! Not Recommneded !!!



Find what strategy was used in update
-------------------------------------

Show deployemnt details
	terminal --> kubectl describe deployment my-depoyment

Under 'StrategyType' field we can see 'Recreate' or 'RollingUpdate' values

Also messages with information about setting replicas to '0' laso means that 'Recreate' strategy was used



How RollingUpdate is working?
-----------------------------

When new deployment is created with 5 replicas, it first creates a ReplicaSet automatically that creates 5 replicas.

When Upgrade is made a new ReplicaSet is created. The two replicasets are working simultaneously. Old ReplicaSet deletes one pod and the new ReplicaSet create one pod and so on.

We can check old and new replicaset object
	terminal --> k get replicasets


How Rollback work?
------------------

Star a rollback
	terminal --> kubectl rollout undo deployment/myapp-deployment

	# the deployment will destroy the pods in the last ReplicaSet and restore the pods in the previous ReplicaSet

We can trace this process by listing ReplicaSets before and after a rollback command
	terminal --> kubectl get replicasets



Commands Summarize
------------------

Craete deployemnt
	terminal --> kubectl create deployment -f deployment-definition.yaml

List deployments
	terminal --> kubectl get deployments

Update deployemnts
	terminal --> kubectl apply -f deployment-definition.yaml			# recommended
	or
	terminal --> kubectl set image deployemnt/myapp-deployment nginx=nginx:1.9.1	# not recommended, DO NOT update def file

Show deployemnt status
	terminal --> kubect rollout status deployemnt/myapp-deployment

Show deployment history
	terminal --> kubectl rollout history deployemnt/myapp-deployment




===================================
Section 6 95. Updating a Deployment
===================================

Here are some handy examples related to updating a Kubernetes Deployment:

Creating a deployment, checking the rollout status and history:
---------------------------------------------------------------
In the example below, we will first create a simple deployment and inspect the rollout status and the rollout history:

master $ --> kubectl create deployment nginx --image=nginx:1.16

	# result: deployment.apps/nginx created
 
master $ --> kubectl rollout status deployment nginx

	# result:
	Waiting for deployment "nginx" rollout to finish: 0 of 1 updated replicas are available...
	deployment "nginx" successfully rolled out
 
 
master $ --> kubectl rollout history deployment nginx

	# result:
	deployment.extensions/nginx
	REVISION CHANGE-CAUSE
	1     <none>
 

Using the --revision flag:
--------------------------
Here the revision 1 is the first version where the deployment was created.

You can check the status of each revision individually by using the --revision flag:

master $ --> kubectl rollout history deployment nginx --revision=1

# result: 
deployment.extensions/nginx with revision #1
Pod Template:
 Labels:    app=nginx    pod-template-hash=6454457cdb
 Containers:  nginx:  Image:   nginx:1.16
  Port:    <none>
  Host Port: <none>
  Environment:    <none>
  Mounts:   <none>
 Volumes:   <none>



Using the --record flag:
------------------------
You would have noticed that the "change-cause" field is empty in the rollout history output. We can use the --record flag to save the command used to create/update a deployment against the revision number.

master $ --> kubectl set image deployment nginx nginx=nginx:1.17 --record

# result:
deployment.extensions/nginx image updated

 
master $ --> kubectl rollout history deployment nginx

# result:
deployment.extensions/nginx
 
REVISION CHANGE-CAUSE
1     <none>
2     kubectl set image deployment nginx nginx=nginx:1.17 --record=true


You can now see that the change-cause is recorded for the revision 2 of this deployment.

Let's make some more changes. In the example below, we are editing the deployment and changing the image from nginx:1.17 to nginx:latest while making use of the --record flag.

master $ --> kubectl edit deployments. nginx --record

# result: deployment.extensions/nginx edited
 

master $ --> kubectl rollout history deployment nginx

# result:
REVISION CHANGE-CAUSE
1     <none>
2     kubectl set image deployment nginx nginx=nginx:1.17 --record=true
3     kubectl edit deployments. nginx --record=true
 
 
master $ --> kubectl rollout history deployment nginx --revision=3

# result: 
deployment.extensions/nginx with revision #3
 
Pod Template: Labels:    app=nginx
    pod-template-hash=df6487dc Annotations: kubernetes.io/change-cause: kubectl edit deployments. nginx --record=true
 
 Containers:
  nginx:
  Image:   nginx:latest
  Port:    <none>
  Host Port: <none>
  Environment:    <none>
  Mounts:   <none>
 Volumes:   <none>
 


Undo a change:
--------------
Lets now rollback to the previous revision:

controlplane $ --> kubectl rollout history deployment nginx

# result:
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none>
3         kubectl edit deployments.apps nginx --record=true
4         kubectl set image deployment nginx nginx=nginx:1.17 --record=true
 
 
 
controlplane $ --> kubectl rollout history deployment nginx --revision=3

# result:
deployment.apps/nginx with revision #3
Pod Template:
  Labels:       app=nginx
        pod-template-hash=787f54657b
  Annotations:  kubernetes.io/change-cause: kubectl edit deployments.apps nginx --record=true
  Containers:
   nginx:
    Image:      nginx:latest
    Port:      <none> 
    Host Port:  <none>
    Environment: <none>       
    Mounts:     <none>
  Volumes:      
 
controlplane $ --> kubectl describe deployments. nginx | grep -i image:

# result:
    Image:        nginx:1.17
 



With this, we have rolled back to the previous version of the deployment with the image = nginx:1.17.

controlplane $ --> kubectl rollout history deployment nginx --revision=1

# result:
deployment.apps/nginx with revision #1
Pod Template:
  Labels:       app=nginx
        pod-template-hash=78449c65d4
  Containers:
   nginx:
    Image:      nginx:1.16
    Port:       <none> 
    Host Port:  <none>
    Environment: <none>     
    Mounts:     <none>
  Volumes:      
 

controlplane $ --> kubectl rollout undo deployment nginx --to-revision=1

# result:
deployment.apps/nginx rolled back
To rollback to specific revision we will use the --to-revision flag.
With --to-revision=1, it will be rolled back with the first image we used to create a deployment as we can see in the rollout history output.

controlplane $ --> kubectl describe deployments. nginx | grep -i image:

# result:
Image: nginx:1.16



=============================
Section 6 96. Demo Deployment
=============================

We will look at updates and rollbacks in deployments

Show all existing objects on the node
	master node terminal --> kubectl get all
	
	# result: 
	NAME 		TYPE		CLUSTER-IP 	EXTERNAL-IP	PORT(S)		AGE
	svc/kubernetes	ClusterIP	10.96.0.1	<none>		443/TCP		2d


Create new deployment
	master node terminal --> kubectl create -f deployment-definition.yaml
	
	# result: deployment "myapp-deployment" created


Show update status - creates rollouts
	master node terminal --> kubectl rollout status deployment/myapp-deployment

	# result:
	Waiting for rollout to finish: 2 of 6 updated replicas are available...
	Waiting for rollout to finish: 3 of 6 updated replicas are available...
	Waiting for rollout to finish: 4 of 6 updated replicas are available...
	Waiting for rollout to finish: 5 of 6 updated replicas are available...
	deployment "myapp-deployment" successfully rolled out


Show update history
	master node terminal --> kubectl rollout history deployment/myapp-deployment

	# result:
	deployments "myapp-deployment"
	REVISION	CHANGE-CAUSE
	1		<none>

If we delete the deployment
	master node terminal --> kubectl delete deployment myapp-deployment
	
	# result: deployment "myapp-deployment" deleted

Wait until all objects connected with the deployemnt to be deleted.

And recreate the deployment with additional command flag (--record)
	master node terminal --> k create -f deployment-definition.yaml --record

	# result: "myapp-deployment" created

Show deployment status
	master node terminal --> kubectl rollout status deployment/myapp-deployment

	# result:
	Waiting for rollout to finish: 1 of 6 updated replicas are available...
	Waiting for rollout to finish: 2 of 6 updated replicas are available...
	Waiting for rollout to finish: 3 of 6 updated replicas are available...
	Waiting for rollout to finish: 4 of 6 updated replicas are available...
	Waiting for rollout to finish: 5 of 6 updated replicas are available...
	deployment "myapp-deployment" successfully rolled out


Show deployment history
	master node terminal --> kubectl rollout history deployment/myapp-deployment

	# result:
	deployments "myapp-deployment"
	REVISION	CHANGE-CAUSE
	1		kubectl create --filename=deployment-definition.yaml --record=true


We will make a small change to our deployment - downgrade of the used nginx image (because by default it takes the latest version)
	terminal --> vi deployment-definition.yaml

deployment-definition.yaml
-------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
	app: myapp

    spec:
      containers:
	- name: nginx-container
	  image: nginx:1.12			# added ':1.12'

  replicas: 6

  selector:
    matchLabels:
      app: myapp
-------------------------------------
save changes - escape, :wq!, enter


Apply the changes of the deployment
	terminal --> kubectl apply -f deployment-definition.yaml
	
	# result: deployment "myapp-deployment" configured


Show deployment status
	master node terminal --> kubectl rollout status deployment/myapp-deployment

	# result:
	Waiting for rollout to finish: 3 of 6 updated replicas are updated...
	Waiting for rollout to finish: 4 of 6 updated replicas are updated...
	Waiting for rollout to finish: 5 of 6 updated replicas are updated...
	...
	deployment "myapp-deployment" successfully rolled out


List deployments
	master node terminal --> kubectl get deployment

	# result:
	NAME			DESIRED		CURRENT		UP-TO-DATE	AVAILABLE	AGE
	myapp-deployment	6		6		6		6		4m


List pods
	master node terminal --> kubectl get pods


Show deployment details
	master node terminal --> kubectl describe deploymnet

	# we can see the image in 'Image: nginx:1.12' line


Show deployment history again
	master node terminal --> kubectl rollout history deployment/myapp-deployment

	# result:
	deployments "myapp-deployment"
	REVISION	CHANGE-CAUSE
	1		kubectl create --filename=deployment-definition.yaml --record=true
	2		kubectl apply --filename=deployment-definition.yaml --record=true	

	# We can see that new revision is added with the executed command



Now we will change the image (nginx:1.12-perl) with imperative approach - with one command
	master node terminal --> kubectl set image deployment/myapp-deployment nginx-container=nginx:1.12-perl

	# result: deployment "myapp-deployment" image updated

	# This command DO NOT update the deployment definition file !


Create new rollout
	master node terminal --> kubectl rollout status deployment/myapp-deployment

	# result:
	Waiting for rollout to finish: 3 of 6 updated replicas are updated...
	Waiting for rollout to finish: 4 of 6 updated replicas are updated...
	Waiting for rollout to finish: 5 of 6 updated replicas are updated...
	...
	deployment "myapp-deployment" successfully rolled out


Show deployment history after imperative approach update
	master node terminal --> kubectl rollout history deployment/myapp-deployment

	# result:
	deployments "myapp-deployment"
	REVISION	CHANGE-CAUSE
	1		kubectl create --filename=deployment-definition.yaml --record=true
	2		kubectl apply --filename=deployment-definition.yaml --record=true
	3		kubectl set image deployment/myapp-deployment nginx-container=nginx:1.12-perl

	# We can see that new revision with the executed command is added


Show deployment details
	master node terminal --> kubectl describe deploymnet

	# we can see the image in 'Image: nginx:1.12-perl' line



Now we will rollback the last change
	master node terminal --> kubectl rollout undo deployment/myapp-deployment

	# result: deployment "myapp-deployment"


Show deployment status
	master node terminal --> kubectl rollout status deployment/myapp-deployment

	# result:
	Waiting for rollout to finish: 3 of 6 updated replicas are updated...
	Waiting for rollout to finish: 4 of 6 updated replicas are updated...
	Waiting for rollout to finish: 5 of 6 updated replicas are updated...
	...
	deployment "myapp-deployment" successfully rolled out


Show deployment history after rollback
	master node terminal --> kubectl rollout history deployment/myapp-deployment

	# result:
	deployments "myapp-deployment"
	REVISION	CHANGE-CAUSE
	1		kubectl create --filename=deployment-definition.yaml --record=true
	3		kubectl set image deployment/myapp-deployment nginx-container=nginx:1.12-perl
	4		kubectl apply --filename=deployment-definition.yaml --record=true

	# We can see that old revision is deleted (2) and added as new one (4) with the executed command


Show deployment details
	master node terminal --> kubectl describe deploymnet

	# we can see the updated image in 'Image: nginx:1.12' line is back to previous version - 2



We will simulate error by setting image of the nginx that do not exist
----------------------------------------------------------------------

Edit the deployment definition file
	master node terminal --> vi deployment-definition.yaml

deployment-definition.yaml
-------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
	app: myapp

    spec:
      containers:
	- name: nginx-container
	  image: nginx:1.12			# changed to 1.5-err	

  replicas: 6

  selector:
    matchLabels:
      app: myapp
-------------------------------------
save changes - escape, :wq!, enter


Try to apply changes
	master node terminal --> kubectl apply -f deployment-definition.yaml --record

	# result: deployment "myapp-deployment" configured


Monitor the status of the update
	master node terminal --> kubectl rollout status deployment/myapp-deployment

	# result:
	Waiting for rollout to finish: 3 of 6 updated replicas are updated...
	
	# update is stuck
	# escape the monitoring with ctrl+c


List deployments
	master node terminal --> kubectl get deployment

	# result:
	NAME			DESIRED		CURRENT		UP-TO-DATE	AVAILABLE	AGE
	myapp-deployment	6		8		3		5		11m

	# trying to update but do not finds the image


We can list the pods to check on the status of every one of them
	master node terminal --> kubectl get pods

	# result:
	NAME					READY		STATUS			RESTARTS	AGE
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	0/1		ImagePullBackoff	0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	0/1		ImagePullBackoff	0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	0/1		ImagePullBackoff	0		4m

	# We can see that 3 of the pods are in not ready state with status 'ImagePullBackoff'
	# Kubernetes acts proactivelly
		- teminates only one of the working pods
		- tryied to create 3 new pods with the wrong image and stop


Show deployment history
	master node terminal --> kubectl rollout history deployment/myapp-deployment

	# result:
	deployments "myapp-deployment"
	REVISION	CHANGE-CAUSE
	1		kubectl create --filename=deployment-definition.yaml --record=true
	3		kubectl set image deployment/myapp-deployment nginx-container=nginx:1.12-perl
	4		kubectl apply --filename=deployment-definition.yaml --record=true
	5		kubectl apply --filename=deployment-definition.yaml --record=true

	# We can see that new revision is created (5) with the executed command


We now will rollback (undo) the last image change (with the wrong image) and see the result - rollback
	master node terminal --> kubectl rollout undo deployment/myapp-deployment

	# result: deployment "myapp-deployment"


Show deployment history after rollback
	master node terminal --> kubectl rollout history deployment/myapp-deployment

	# result:
	deployments "myapp-deployment"
	REVISION	CHANGE-CAUSE
	1		kubectl create --filename=deployment-definition.yaml --record=true
	3		kubectl set image deployment/myapp-deployment nginx-container=nginx:1.12-perl
	4		kubectl apply --filename=deployment-definition.yaml --record=true
	5		kubectl apply --filename=deployment-definition.yaml --record=true
	6		kubectl apply --filename=deployment-definition.yaml --record=true

	# We can see that new revision is created (6) with the executed command


Show deployment details
	master node terminal --> kubectl describe deploymnet

	# we can see the updated image in 'Image: nginx:1.12' line is back to previous version


We can list the pods 
	master node terminal --> kubectl get pods

	# result:
	NAME					READY		STATUS			RESTARTS	AGE
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m
	myapp-deployment-xxxxxxxxxx-xxxxx	1/1		Running			0		4m

	# now all pods are in runnign state


========================================================
Section 6 98. Practice Test - Rolling Updates & Rollback
========================================================

We will use 'k' alias for 'kubectl' command. We can see all alias with
	terminal --> alias 

1. We have deployed a simple web application. Inspect the PODs and the Services
-------------------------------------------------------------------------------
Wait for the application to fully deploy and view the application using the link called Webapp Portal above your terminal.

List pods
	terminal --> k get pods

List deployemnts
	terminal --> k get deploy

Click on 'Webapp Portal' above top right corner of the console
	- window with the working app should open

- click 'Ok' button



2. What is the current color of the web application?
----------------------------------------------------
Access the Webapp Portal.

Click on 'Webapp Portal' above top right corner of the console
	- window with the working app should open

- choose 'blue' as answer (or whatever color is the background of the application)



3. Run the script named curl-test.sh to send multiple requests to test the web application. Take a note of the output.
----------------------------------------------------------------------------------------------------------------------
Execute the script at /root/curl-test.sh.


Run command:
	terminal --> ./curl-test.sh

	# result:
	Hello, Application Version: v1 ; Color: blue OK

	Hello, Application Version: v1 ; Color: blue OK

	Hello, Application Version: v1 ; Color: blue OK
	...

- click 'Ok' button




4. Inspect the deployment and identify the number of PODs deployed by it
------------------------------------------------------------------------

List deployments
	terminal --> k get deploy

	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   4/4     4            4           106s

- choose '4' as answer



5. What container image is used to deploy the applications?
-----------------------------------------------------------

List deployemnts
	terminal --> k get deploy

	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   4/4     4            4           2m27s

Show used image in the target deployemnt
	terminal --> k describe deploy frontend | grep Image

	# result:     Image:         kodekloud/webapp-color:v1

OR 

List pods
	terminal --> k get pods

Show image used in one of the pods
	terminal --> k describe pod frontend-7b5df69f4-bms77 | grep Image

- choose 'kodekloud/webapp-color:v1' as answer



6. Inspect the deployment and identify the current strategy
-----------------------------------------------------------

List deployemnts
	terminal --> k get deploy

Show used strategy in the target deployemnt
	terminal --> k describe deploy frontend | grep StrategyType

- choose 'RollingUpdate' as answer



7. If you were to upgrade the application now what would happen?
----------------------------------------------------------------

- choose 'PODs are upgraded few at a time' as answer



8. Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2
---------------------------------------------------------------------------------------------------------------
Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

Task approach
	1. Edit the existing deployment and apply changes

List deployments
	terminal --> k get deploy

	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   4/4     4            4           5m49s


Option 1:
---------
Update deployemnt configs
	terminal --> k edit deploy frontend

frontedn deployment
------------------------------------------------
...
spec:
  minReadySeconds: 20
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2			# changed
        imagePullPolicy: IfNotPresent
...
------------------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/frontend edited

Verify the change
	terminal --> k describe deploy frontend

	# We can see changed line 'Image:         kodekloud/webapp-color:v2'


Option 2:
---------
Show details of the deployment and copy the container name
	terminal --> k describe deployment frontend

	# result:
	-----------------
	...
	  Containers:
	   simple-webapp:
	...
	-----------------

	# container name is 'simple-webapp'


Update the deployment without updating the deployment config
	terminal --> k set image deploy frontend simple-webapp=kodekloud/webapp-color:v2

	# k 						- common kubernetes command
	# set image					- update deployment image
	# deploy					- type object
	# frontend					- target object name
	# simple-webapp=kodekloud/webapp-color:v2	- container name and imagename

Verify the change
	terminal --> k describe deploy frontend


- click 'Check' button




9. Run the script curl-test.sh again. Notice the requests now hit both the old and newer versions. However none of them fail.
-----------------------------------------------------------------------------------------------------------------------------
Execute the script at /root/curl-test.sh.

Run tests
	terminal --> ./curl-test.sh

	# result:
	Hello, Application Version: v2 ; Color: green OK

	Hello, Application Version: v2 ; Color: green OK

	Hello, Application Version: v2 ; Color: green OK
	...

	# we can see that there is a new colors in the tests results
	# if we run the tests few times, all colors wull be changed 
	# that is how rollingupdate work

- click 'Ok' button



10 . Up to how many PODs can be down for upgrade at a time
----------------------------------------------------------
Consider the current strategy settings and number of PODs - 4

List pods
	terminal --> k get pods

	# result:
	NAME                        READY   STATUS    RESTARTS   AGE
	frontend-854b57fbbf-ccrbc   1/1     Running   0          6m56s
	frontend-854b57fbbf-fts95   1/1     Running   0          6m34s
	frontend-854b57fbbf-nqzpn   1/1     Running   0          6m56s
	frontend-854b57fbbf-tws7p   1/1     Running   0          6m34s

	# we have 4 pods


Show details about the deployment
	terminal --> k describe deploy frontend

# in the result We have fields:
-----------------------------------
...
	MinReadySeconds:        20
	RollingUpdateStrategy:  25% max unavailable, 25% max surge
...
-----------------------------------

- choose '1' as answer



11. Change the deployment strategy to Recreate
----------------------------------------------
Delete and re-create the deployment if necessary. Only update the strategy type for the existing deployment.


List deployemnts
	terminal --> k get deploy
	
	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   4/4     4            4           15m


Update deployemnt configs
	terminal --> k edit deploy frontend

frontedn deployment
------------------------------------------------
...
spec:
  minReadySeconds: 20
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp
  strategy:						# removed additional lines
    type: Recreate					# changed
  template:
...
------------------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/frontend edited


Verify the change
	terminal --> k describe deploy frontend | grep StrategyType

	# result: StrategyType:       Recreate


- click 'Check' button




12. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v3
-----------------------------------------------------------------------------------------------
Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

List deployments
	terminal --> k get deploy

	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   4/4     4            0           20m


Option 1:
---------
Update deployemnt configs
	terminal --> k edit deploy frontend

frontedn deployment
------------------------------------------------
...
spec:
  minReadySeconds: 20
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v3			# changed		
        imagePullPolicy: IfNotPresent
...
------------------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/frontend edited


Verify the change
	terminal --> k describe deploy frontend | grep Image

	# result:     Image:         kodekloud/webapp-color:v3


Option 2:
---------
Show details of the deployment and copy the container name
	terminal --> k describe deployment frontend

	# container name is 'simple-webapp'

Update the deployment without updating the deployment config
	terminal --> k set image deploy frontend simple-webapp=kodekloud/webapp-color:v3

	# k 						- common kubernetes command
	# set image					- update deployment image
	# deploy					- type object
	# frontend					- target object name
	# simple-webapp=kodekloud/webapp-color:v3	- container name and imagename

Verify the change
	terminal --> k describe deploy frontend | grep Image

	# result:     Image:         kodekloud/webapp-color:v3


- click 'Check' button



13. Run the script curl-test.sh again. Notice the failures. Wait for the new application to be ready. Notice that the requests now do not hit both the versions
----------------------------------------------------------------------------------------------------------------------------------
Execute the script at /root/curl-test.sh.


Run tests
	terminal --> ./curl-test.sh

	# result:
	Hello, Application Version: v3 ; Color: red OK

	Hello, Application Version: v3 ; Color: red OK

	Hello, Application Version: v3 ; Color: red OK
	...

	# we can see that all test result changed to red at once
	# that is how recrate strategy work

- click 'Ok' button



==============================================
Section 6 99. Deployment Strategy - Blue Green
==============================================

Deployment strategies
	1. Recreate - destroying all existing versions instances and then deploying new versions instances
		- CON - creates downtime, not accessabel for users
	2. Rollin Update - Destroy old version isntances and create new version instances one by one (default kubernetes strategy)
	3. Blue/Green
	4. Canary


Blue/Green deployment strategy
==============================

We have old deployemnt (Blue) with old version instances. We create new deployment (Green) with new version instances. We perform tests on the new deployment (Green) and then redirect the traffic from old deployment (Blue) to the new deployment (Green). We can implement this strategy with Istio Service.


How it works
------------

We have (Blue) deployment and service to route traffic to it. To associate the service to the deployment we set a label of the deployment and selector for this label on the service - version: v1. Then we deploy a second (Green) deployment with new version of the application and label - version: v2. We perform tests on the Green deployment to assure that the application is working properly. The we switch traffic from Blue (old) deployment to the Grren (new) deployment by changing the selector of the service from version: v1 to version: v2.

Example with code
-----------------

We have old deployment (Blue):

myapp-blue.yaml
-------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-blue
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
	version: v1

    spec:
      containers:
      - name: app-container
	image: myapp-image:1.0
  replicas: 5
  selector:
    matchLabels:
      version: v1
-------------------------------


service-definition.yaml
-------------------------------
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    version: v1
-------------------------------

Now all user traffic is directed to deployment blue


We deploy a new deployment Green with the new version of the application instances

myapp-green.yaml
-------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-green		# different name
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
	version: v2		# different label

    spec:
      containers:
      - name: app-container
	image: myapp-image:2.0	# different image
  replicas: 5
  selector:
    matchLabels:
      version: v2		# different selector
-------------------------------

We perform all the tests on the deployment to ensure that everything is working propoerly.

To switch the traffic to the deployment we change the selector in the service.

service-definition.yaml
-------------------------------
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    version: v2				# changed selector label
-------------------------------




===========================================
Section 6 100. Deployment Strategy - Canary
===========================================

In this strategy we have primary (main) deployment and new deployment (Canary). Then we route only a small procentage of the traffic to the new Canary deployment. We perform tests on it and if tests passes successfully we update the primary (main) deployemnt with Rollin Update strategy. Then we destroy the Canary deployment.


How it works
------------

We have primary deployment with 5 replicas (pods) and service that routes the traffic to all of the pods. The main deployment have label version: v1 and the service have selector version: v1. We deploy a second deplyment (Canary) with 5 replicas (pods).

We want to acheave 2 things with the new Canary deployment
	1. Traffic should go to both deployments in the same time
	2. We want to route a small procentage of traffic to Canary deployment


1. Make the traffic go to the both deplyments
---------------------------------------------
- We create a common label in both deployemnts - app: front-end
- Modify the selector in the service to match this common label - app: front-end

Now the traffic is routed equally between the deployments - 50% - 50%


2. Reduce the traffic to the Canary (second) deployment
-------------------------------------------------------
- We reduce the pods in the Canary deployemnt to minimum - 1

Since Kubernetes routes the traffic between all pods equally, the rpocentage is 5:1 - 83% - 17%.

Now we can perform tests on the Canary deployment (with one pod). We can update the primary deployment with Rolling Update strategy. Then we can delete the Canary deployment.

One of the cavities of the canary strategy is that we have a limited control over the routed traffic or the split of the traffic between each deployment. The traffic split is always going to be governed by the number of pods present in each deployment. We cannot set 1% of the traffic to be routed to the canary deployment. Solution for this issue is using services like Istio that define the exact precentage of traffic to be distributed between each deployment. It is not dependent on the number of pods in the deployment.



Example with code
-----------------

We have primary deployment:

myapp-primary.yaml
-------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-primary
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
	version: v1
        app: front-end		# set common label

    spec:
      containers:
      - name: app-container
	image: myapp-image:1.0
  replicas: 5
  selector:
    matchLabels:
        app: front-end		# set selector label
-------------------------------


We have service:

service-definition.yaml
-------------------------------
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: front-end
-------------------------------


We create Canary deployment

myapp-canary.yaml
-------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-canary		# different name
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
	version: v2		# different version
        app: front-end		# common label	

    spec:
      containers:
      - name: app-container
	image: myapp-image:2.0	# different image version
  replicas: 1			# set minimum replicas
  selector:
    matchLabels:
        app: front-end		# set selector label
-------------------------------




==================================================
Section 6 102. Practice Test - Deployment Strategy
==================================================

1. A deployment has been created in the default namespace. What is the deployment strategy used for this deployment?
--------------------------------------------------------------------------------------------------------------------

List deployments
	terminal --> k get deploy

	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   5/5     5            5           26s


Show deployment details
	terminal --> k describe deploy frontend 

# result:
-----------------------------------
...
StrategyType:           RollingUpdate
...
-----------------------------------

- choose 'Rolling Update' as answer




2. The deployment called frontend app is exposed on the NodePort via a service.
-------------------------------------------------------------------------------
Identify the name of this service.

List services
	terminal --> k get svc

	# result:
	NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
	frontend-service   NodePort    172.20.223.70   <none>        8080:30080/TCP   117s	# this is the service
	kubernetes         ClusterIP   172.20.0.1      <none>        443/TCP          7m42s

We can check service selector to verify the deployment connection
	termianl --> k describe svc frontend-service

# result:
-----------------------------------
...
Selector:                 app=frontend
...
-----------------------------------

Show deployment selector
	terminal --> k describe deploy frontend | grep Selector

	# result:
	Selector:               app=frontend	# same as in the service
  	  Node-Selectors:  <none>

- choose 'frontend-service' as answer



3. What is the selector used by this service?
---------------------------------------------
Inspect the service called frontend-service.


Show service selector 
	termianl --> k describe svc frontend-service

# result:
-----------------------------------
...
Selector:                 app=frontend
...
-----------------------------------

- choose 'app: frontend' as answer




4. Check out the web application using the Webapp link above your terminal.
---------------------------------------------------------------------------

Open the Webapp tab above the terminal

- click 'Ok' button




5. A new deployment called frontend-v2 has been created in the default namespace using the image kodekloud/webapp-color:v2. This deployment will be used to test a newer version of the same app.
--------------------------------------------------------------------------------------------------------------------------------
Configure the deployment in such a way that the service called frontend-service routes less than 20% of traffic to the new deployment.
Do not increase the replicas of the frontend deployment.


List deployemnts
	terminal --> k get deploy
	
	# result:
	NAME          READY   UP-TO-DATE   AVAILABLE   AGE
	frontend      5/5     5            5           8m11s
	frontend-v2   2/2     2            2           28s		# this is the new deployment and it has 2 pods

Edit the deplyment and set replicas to 1
	terminal --> k edit deploy frontend-v2

frontend-v2
---------------------------------
...
spec:
  progressDeadlineSeconds: 600
  replicas: 1				# set replicas to 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: frontend			# check selector
  strategy:
    rollingUpdate:
...
---------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/frontend-v2 edited


Verify changes
	terminal --> k get deploy

	# result:
	NAME          READY   UP-TO-DATE   AVAILABLE   AGE
	frontend      5/5     5            5           10m
	frontend-v2   1/1     1            1           3m4s

- click 'Check' button




6. Check out the new version of the web application using the Webapp link above your terminal.
----------------------------------------------------------------------------------------------
You may need to refresh it multiple times to see the new version.

Open Webapp tab above the terminal and refresh few times till the you see 'Application Version: v2'

- click 'Ok' button



7. We have now established that the new version v2 of the application is working as expected.
---------------------------------------------------------------------------------------------
We can now safely redirect all users to the v2 version.

- click 'Ok' button



8. Scale down the v1 version of the apps to 0 replicas and scale up the new(v2) version to 5 replicas.
------------------------------------------------------------------------------------------------------

List deployments
	terminal --> k get deploy

	# result:
	NAME          READY   UP-TO-DATE   AVAILABLE   AGE
	frontend      5/5     5            5           18m
	frontend-v2   1/1     1            1           10m

We can scale the deployments by 2 options
	1. Edit the deployments and modify the replicas to 0 for forntend and to 5 for frontend-v2
	2. Scale down both deployments with one line command

Opton 1
-------

Edit deployment frontend and set replicas to 0
	terminal --> k edit deploy frontend

frontend
---------------------------------------
...
spec:
  progressDeadlineSeconds: 600
  replicas: 0				# set replicas to 0
  revisionHistoryLimit: 10
  selector:
...
---------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/frontend edited

Edit deployment frontend and set replicas to 5
	terminal --> k edit deploy frontend-v2

frontend-v2
---------------------------------------
...
spec:
  progressDeadlineSeconds: 600
  replicas: 5				# set replicas to 5
  revisionHistoryLimit: 10
...
---------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/frontend-v2 edited

List deployments to verify the scaling
	terminal --> k get deploy

	# result:
	NAME          READY   UP-TO-DATE   AVAILABLE   AGE
	frontend      0/0     0            0           21m	# 0 pods
	frontend-v2   5/5     5            5           13m	# 5 pods


Option 2
--------

Scale deployemnts with one command

Scale deployment frontend to 0 replicas
	terminal --> k scale deploy frontend --replicas=0

	# result: deployment.apps/frontend scaled

Scale deployment frontend-v2 to 5 replicas
	terminal --> k scale deploy frontend-v2 --replicas=4

	# result: deployment.apps/frontend-v2 scaled


- click 'Check' button



9. Now delete the deployment called frontend completely.
--------------------------------------------------------

List deployments
	terminal --> k get deploy

	# result:
	NAME          READY   UP-TO-DATE   AVAILABLE   AGE
	frontend      0/0     0            0           26m	# deployment to delete
	frontend-v2   5/5     5            5           19m

Delete the deployment
	terminal --> k delete deploy frontend

	# result: deployment.apps "frontend" deleted

Verify deployment deletion
	terminal --> k get deploy
	
	# result:
	NAME          READY   UP-TO-DATE   AVAILABLE   AGE
	frontend-v2   5/5     5            5           20m

- click 'Check' button



10. You can now reload the Webapp tab to validate that all user traffic now uses the v2 version of the app.
-----------------------------------------------------------------------------------------------------------

Open the Webapp page above the termianl and check the version. Refresh few times, should be always Application Version: v2

- click 'Check' button



===================
Section 6 103. Jobs
===================

There are different workload that a container can serve - Web Servers, Applications, DBs. There are other kinds of workloads such as Batch processing, analytics or reporting that are ment to carry out a specific taks and then finish.

Example for temporary tasks are claculating/computing, image processing or perofrming some kind of alanytics on large dataset, generating a report and sending an email, etc. Those are workloads that are ment to live for a short period of time. Perform a set of tasks and then finish.


How such a workload work in Docker
----------------------------------

Start a Docker container to perform a simple calculation
	terminal --> docker run ubuntu expr 3 + 2

The container start up, perform the calculation and exits

When we list containers
	terminal --> docker ps -a

We can see that the container have STATUS 'Exited (0)'. Code 0 means that the operation is successful.



How to replicate the same in Kubernetes
---------------------------------------

We create a pod-definition file and create a pod.

pod-definition.yaml
--------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: math-pod
spec:
  containers:
  - name: math-add
    image: ubuntu
    command: ["expr", "3", "+", "2"]
--------------------------------

Create the pod to perform the operation
	terminal --> k create -f pod-definition.yaml


List pods
	terminal --> k get pods

	# result:
	NAME		READY		STATUS		RESTARTS		AGE
	math-pod	0/1		Completed	0			1d

	# result 2:
	NAME		READY		STATUS		RESTARTS		AGE
	math-pod	0/1		Completed	1			1d

	# result 3:
	NAME		READY		STATUS		RESTARTS		AGE
	math-pod	0/1		Completed	2			1d

	# result 4:
	NAME		READY		STATUS		RESTARTS		AGE
	math-pod	0/1		Completed	3			1d


Kubernetes restart the container in attempt to keep it running. Restarting stops when the treshold is reached. This behavior is defined by the property 'restartPolicy: Always' set by default in the pod definition file. That is why the pod always recreate the container when it exits. We can overwrite this property to 'Never' to prevent the restarting.

pod-definition.yaml
--------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: math-pod
spec:
  containers:
  - name: math-add
    image: ubuntu
    command: ["expr", "3", "+", "2"]
  restartPolicy: Never
--------------------------------

Container restart policy
	- Always: Automatically restarts the container after any termination.
	- OnFailure: Only restarts the container if it exits with an error (non-zero exit status).
	- Never: Does not automatically restart the terminated container.



We have scenario that we need to use multiple pods to process data in parallel. We need to assure that the pods perform the tasks assigned to them successfully and then exit. So we need a manager to create as many pods as we want and ensure that the work is done successfully.

Like ReplicaSets that maintainn number of pods running in all time, a Job is used to run a set of pods to perform a given task to completion.


Create a Job
============

We create a job definition file

job-definition.yaml
--------------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:
  template:
    containers:
    - name: math-add
      image: ubuntu
      command: ["expr", "3", "+", "2"]
    restartPolicy: Never
--------------------------------

Create the job
	terminal --> k create -f job-definition.yaml

List jobs
	terminal --> k get jobs

	# result:
	NAME		DESIRED		SUCCESSFUL		AGE
	math-add-job	1		1			38s

List pods related to the job
	terminal --> k get pods

	# result:
	NAME			READY		STATUS		RESTARTS	AGE
	math-add-job-ls34pn	0/1		Completed	0		2m


We can see the output of the job in the logs of the pod
	terminal --> k logs math-add-job-ls34pn
	
	# result: 5

Delete the job
	terminal --> k delete job math-add-job

	# result: job.batch "math-add-job" deleted

The deletion of the job also deletes all related pods!


In real world case of image processing, the result will be saved in persisten volume. Or if the Job is to generate a report or send an email, then the email and the report would be the result of the job.



How to run multiple pods Job for task
-------------------------------------

We define 'completions: 3' property with the number of job completions.

job-definition.yaml
--------------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:
  completions: 3			# set how many time the task should be completed
  template:
    containers:
    - name: math-add
      image: ubuntu
      command: ["expr", "3", "+", "2"]
    restartPolicy: Never
--------------------------------


Create the Job
	terminal --> k create -f job-definition.yaml

List Jobs
	terminal --> k get jobs

	# result:
	NAME		DESIRED		SUCCESSFUL		AGE
	math-add-job	3		3			38s

List pods related to the job
	terminal --> k get pods

	# result:
	NAME			READY		STATUS		RESTARTS	AGE
	math-add-job-ls34pn	0/1		Completed	0		2m	# 1 completed
	math-add-job-fa28sg	0/1		Completed	0		2m	# 2 completed
	math-add-job-s4sfc3	0/1		Completed	0		2m	# 3 completed

By default the pods are created one after another. The second pod is created only after the first is finished.




Scenario with failure pods
--------------------------

job-definition.yaml
--------------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:
  completions: 3				# completions required - 3
  template:
    containers:
    - name: random-error	
      image: kodekloud/random-error		# simple docker image that randomly completes or fails
    restartPolicy: Never
--------------------------------

Create the job
	terminal --> k craete -f job-definition.yaml

List jobs
	terminal --> k get jobs

	# result:
	NAME		DESIRED		SUCCESSFUL		AGE
	math-add-job	3		0			38s


List pods related to the job
	terminal --> k get pods

	# result:
	NAME			READY		STATUS		RESTARTS	AGE
	math-add-job-ls34pn	0/1		Completed	0		2m	# 1 completed	
	math-add-job-fa28sg	0/1		Error		0		2m	# 2 completed	# first error
	math-add-job-s4sfc3	0/1		Completed	0		2m	# 3 completed

	math-add-job-s43sfs	0/1		Error		0		2m	# 2 completed	# second error
	math-add-job-75dfds	0/1		Error		0		2m	# 2 completed	# third error
	math-add-job-d343df	0/1		Completed	0		2m	# 3 completed	# final competion


Kubernetes creates new pods in order to complete successful the tasks required times.



Parallel jobs
-------------

We can set parallel jobs to perform the task for short time.

job-definition.yaml
--------------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:
  completions: 3				# completions required - 3
  parallelism: 3				# create 3 pods in parallel to perform the job
  template:
    containers:
    - name: random-error	
      image: kodekloud/random-error		# simple docker image that randomly completes or fails
    restartPolicy: Never
--------------------------------


Create the job
	terminal --> k craete -f job-definition.yaml

List jobs
	terminal --> k get jobs

	# result:
	NAME		DESIRED		SUCCESSFUL		AGE
	math-add-job	3		0			38s


List pods related to the job
	terminal --> k get pods

	# result:
	NAME			READY		STATUS		RESTARTS	AGE
	math-add-job-ls34pn	0/1		Completed	0		2m	# 1 completed	
	math-add-job-fa28sg	0/1		Error		0		2m	# 2 completed	# failed pod
	math-add-job-s4sfc3	0/1		Completed	0		2m	# 3 completed

	# First 3 pods are created at once, then new pods are created one by one to reach the required completions

	math-add-job-s43sfs	0/1		Error		0		2m	# 2 completed	# created second
	math-add-job-75dfds	0/1		Error		0		2m	# 2 completed	# created third
	math-add-job-d343df	0/1		Completed	0		2m	# 3 completed	# created fourth



=======================
Section 6 104. CronJobs
=======================

CronJobs are scheduled jobs (like crontab in Linux).


Let say we have a job that create a report and send an email.

cron-job-definition.yaml
--------------------------------
apiVersion: batch/v1
kind: CronJob
metadata:
  name: reporting-cron-job
spec:
  schedule: "*/1 * * * *"			# schedule syntax
  jobTemplate:
    spec:
      completions: 3				
      parallelism: 3				
      template:
        containers:
        - name: reporting-tool
          image: reporting-tool
        restartPolicy: Never
--------------------------------

We can find CronJob example foramt - https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#example
We can find schedule syntax info - https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule-syntax


Craete cron job
	terminal --> k create -f cron-job-definition.yaml

List cronjobs
	terminal --> k get cronjob

	# result:
	NAME 			SCHEDULE	SUSPEND		ACTIVE
	reporting-cron-job	*/1 * * * *	False		0



=======================================
Section 6 106. Practice Test - CronJobs
=======================================

1. A pod definition file named throw-dice-pod.yaml is given. The image throw-dice randomly returns a value between 1 and 6. 6 is considered success and all others are failure. Try deploying the POD and view the POD logs for the generated number.
--------------------------------------------------------------------------------------------------------------------------------
File is located at /root/throw-dice-pod.yaml


Print the pod definition file
	terminal --> cat /root/throw-dice-pod.yaml

# result:
----------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: throw-dice-pod
spec:
  containers:
  -  image: kodekloud/throw-dice
     name: throw-dice
  restartPolicy: Never
----------------------------------


Create the pod
	terminal --> k create -f throw-dice-pod.yaml

	# result: pod/throw-dice-pod created

List pods
	terminal --> k get pods

	# result:
	NAME             READY   STATUS      RESTARTS   AGE
	throw-dice-pod   0/1     Completed   0          37s	# in completed state

Show pod logs
	terminal --> k logs throw-dice-pod
	
	# result: 6


- click 'Check' button




2. Create a Job using this POD definition file or from the imperative command and look at how many attempts does it take to get a '6'.
---------------------------------------------------------------------------------------------------------------------------------
Use the specification given on the below.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice


Find Job definition format - https://kubernetes.io/docs/concepts/workloads/controllers/job/#running-an-example-job


Create a job definition file
	terminal --> vi throw-dice-job.yaml

throw-dice-job.yaml
-------------------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job			# set required name
spec:
  template:
    spec:
      containers:
      - name: throw-job			# set appropriate name
        image: kodekloud/throw-dice	# set required image
      restartPolicy: Never
  backoffLimit: 25			# set 25 attempts to assure completion
-------------------------------------
save changes - escape, :wq!, enter

Create the job
	termianl --> k create -f throw-dice-job.yaml

	# result: job.batch/throw-dice-job created

List jobs
	terminal --> k get jobs

	# result:
	NAME             STATUS     COMPLETIONS   DURATION   AGE
	throw-dice-job   Complete   1/1           35s        44s	# completed

Show job details
	terminal --> k describe job throw-dice-job

# result:
---------------------------
...
Pods Statuses:    0 Active (0 Ready) / 1 Succeeded / 2 Failed		# we see 2 times failed and 1 succeeded
...
---------------------------

- click 'Check' button




3. Monitor and wait for the job to succeed. Throughout this practice test remember to increase the 'BackOffLimit' to prevent the job from quitting before it succeeds.
--------------------------------------------------------------------------------------------------------------------------------
Check out the documentation page about the BackOffLimit property.

We already increased the BackOffLimit count and the task was completed.

- click 'Check' button




4. How many attempts did it take to complete the job?
-----------------------------------------------------


List jobs
	terminal --> k get jobs

	# result:
	NAME             STATUS     COMPLETIONS   DURATION   AGE
	throw-dice-job   Complete   1/1           35s        44s	# completed

Show job details
	terminal --> k describe job throw-dice-job

# result:
---------------------------
...
Pods Statuses:    0 Active (0 Ready) / 1 Succeeded / 2 Failed	# we see 2 times failed and 1 succeeded - 3 in tital
...
---------------------------

- choose '3' as answer




5. Update the job definition to run as many times as required to get 2 successful 6's.
--------------------------------------------------------------------------------------
Delete existing job and create a new one with the given spec. Monitor and wait for the job to succeed.


List jobs
	terminal --> k get jobs

	# result:
	NAME             STATUS     COMPLETIONS   DURATION   AGE
	throw-dice-job   Complete   1/1           35s        6m14s

Delete the job
	terminal --> k delete job throw-dice-job

	# result: job.batch "throw-dice-job" deleted

List files
	terminal --> ls

	# result: throw-dice-job.yaml  throw-dice-pod.yaml

Edit the job definition file and modify the completions required
	terminal --> vi throw-dice-job.yaml

throw-dice-job.yaml
---------------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 25
  completions: 2			# add completions property with value 2
---------------------------------
save changes - escape, :wq!, enter


Create the job with modified definition file
	terminal --> k create -f throw-dice-job.yaml

	# result: job.batch/throw-dice-job created

List jobs
	terminal --> k get jobs

	# result:
	NAME             STATUS    COMPLETIONS   DURATION   AGE
	throw-dice-job   Running   1/2           15s        15s

Wait and keep listing the jobs until the completions are 2/2


- click 'Check' button




6. How many attempts did it take to complete the job this time?
---------------------------------------------------------------


Show details of the job to see the attempts
	terminal --> k describe job throw-dice-job

----------------------------
...
Pods Statuses:    0 Active (0 Ready) / 2 Succeeded / 7 Failed		# we can see the attempts
...
----------------------------


- choose '9' as answer




7. That took a while. Let us try to speed it up, by running upto 3 jobs in parallel.
------------------------------------------------------------------------------------
Update the job definition to run 3 jobs in parallel.


List jobs
	terminal --> k get jobs

	# result:
	NAME             STATUS     COMPLETIONS   DURATION   AGE
	throw-dice-job   Complete   2/2           10m        15m

Delete the job
	terminal --> k delete job throw-dice-job

	# result: job.batch "throw-dice-job" deleted

List files
	terminal --> ls

	# result: throw-dice-job.yaml  throw-dice-pod.yaml


Edit the job definition file and set parallel propery
	terminal --> vi throw-dice-job.yaml

throw-dice-job.yaml
---------------------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 25
  completions: 3			# set required completions - 3
  parallelism: 3			# added property with required value - 3
---------------------------------------
save changes - escape. :wq!, enter


Create the job
	terminal --> k create -f throw-dice-job.yaml

	# result: job.batch/throw-dice-job created

List jobs
	terminal --> k get jobs

	# result:
	NAME             STATUS    COMPLETIONS   DURATION   AGE
	throw-dice-job   Running   2/3           48s        48s		# we can see completions

Wait and keep listing jobs until completions are 3/3

- click 'Check' button



8. Notice how quickly that finished.
------------------------------------

- clikcl 'Ok' button




9. Let us now schedule that job to run at 21:30 hours every day.		CRON-JOB
----------------------------------------------------------------
Create a CronJob for this.

CronJob Name: throw-dice-cron-job
Image Name: kodekloud/throw-dice
Schedule: 30 21 * * *

We can see cron job definition format - https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#example
We can see the schediule syntax - https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#writing-a-cronjob-spec

Create cron job definition file
	terminal --> vi cron-job.yaml

cron-job.yaml
------------------------------------
apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: throw-dice-cronjob
            image: kodekloud/throw-dice
          restartPolicy: Never
------------------------------------
save changes - escape, :wq!, enter


Create the cron job
	terminal --> k create -f cron-job.yaml

	# result: cronjob.batch/throw-dice-cron-job created

List cronjobs
	terminal --> k get cronjob

	# result:
	NAME                  SCHEDULE      TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
	throw-dice-cron-job   30 21 * * *   <none>     False     0        <none>          33s

- click 'Check' button


